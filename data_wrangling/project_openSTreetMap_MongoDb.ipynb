{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenStreetMap Project Data Wrangling with MongoDB\n",
    "\n",
    "   Author: Nicolas Luiz Ribeiro Veiga  \n",
    "   github: [nicolaslrveiga](https://github.com/nicolaslrveiga)  \n",
    "   Linkedin: [Nicolas Veiga](https://www.linkedin.com/in/nicolas-veiga-79460487/)\n",
    "\n",
    "### Objective\n",
    "   The goal of this document is to apply Data Wrangling tecniques learned on the course(Data Analyst Nanodegree, module 5) over data from OpenStreetMap.  \n",
    "   First we are going to select an region of the globe, than we are going to analyse its data, do some cleaning and report all problems found. The last step is to convert this data into JSON format and import it to MongoDB from there we are able to properly query the data.  \n",
    "   \n",
    "### Introduction\n",
    "São Paulo is a municipality in the southeast region of Brazil. The metropolis is an alpha global city and the most populous city in Brazil and Americas as well as in the Southern Hemisphere. The municipality is also the largest in the Americas and Earth's 12th largest city proper by population. The city is the capital of the surrounding state of São Paulo, one of 26 constituent states of the republic. It is the most populous and wealthiest city in Brazil. It exerts strong international influences in commerce, finance, arts and entertainment. The name of the city honors the Apostle, Saint Paul of Tarsus. The city's metropolitan area of Greater São Paulo ranks as the most populous in Brazil, the 11th most populous on Earth, and largest Portuguese language-speaking city in the world.\n",
    "\n",
    "Having the largest economy by GDP in Latin America and the Southern Hemisphere, the city is home to the São Paulo Stock Exchange. Paulista Avenue is the economic core of São Paulo. The city has the 11th largest GDP in the world, representing alone 10.7% of all Brazilian GDP and 36% of the production of goods and services in the state of São Paulo, being home to 63% of established multinationals in Brazil, and has been responsible for 28% of the national scientific production in 2005. With a GDP of US$477 billions, the Sao Paulo city alone could be ranked 24th globally compared with countries. (2016 Estimates).\n",
    "\n",
    "The metropolis is also home to several of the tallest skyscraper buildings in Brazil, including the Mirante do Vale, Edifício Itália, Banespa, North Tower and many others. The city has cultural, economic and political influence both nationally and internationally. It is home to monuments, parks and museums such as the Latin American Memorial, the Ibirapuera Park, Museum of Ipiranga, São Paulo Museum of Art, and the Museum of the Portuguese Language. The city holds events like the São Paulo Jazz Festival, São Paulo Art Biennial, the Brazilian Grand Prix, São Paulo Fashion Week and the ATP Brasil Open. The São Paulo Gay Pride Parade rivals the New York City Pride March as the largest gay pride parade in the world. It is headquarters of the Brazilian television networks Band, Gazeta and RecordTV.\n",
    "\n",
    "São Paulo is a cosmopolitan, melting pot city, home to the largest Arab, Italian, and Japanese diasporas, with examples including ethnic neighborhoods of Mercado, Bixiga, and Liberdade respectively. São Paulo is also home to the largest Jewish population in the country and one of the largest urban Jewish populations in the world. In 2016, inhabitants of the city were native to 196 different countries. People from the city are known as paulistanos, while paulistas designates anyone from the state, including the paulistanos. The city is also known for the size of its helicopter fleet, its architecture, gastronomy, severe traffic congestion and skyscrapers. [Wiki](https://en.wikipedia.org/wiki/S%C3%A3o_Paulo)\n",
    "   \n",
    "   Furthermore the city is spectacular, so to prove all its glamour we can answer some questions drawed from introduction. The questions we are going to answer are:\n",
    "   - How tall are the 5 tallest buildings?\n",
    "   - With kind of amenity is most common?\n",
    "   - With kind of cousine is predominant?\n",
    "   - With avenue pass by a large number of subburbs?\n",
    "   - How many places to land my helicopter? :)\n",
    "   - I am a big fan of coffe places, how many coffe places I have im my disposal?\n",
    "   \n",
    "   This document is divided as follows: \n",
    "   \n",
    "   \n",
    "   - Dataset:  \n",
    "      - Indicates the dataset used.\n",
    "      - Link to the dataset.  \n",
    "      \n",
    "      \n",
    "   - Data Exploration:\n",
    "      - Basics statistics about the data, like its lenght.\n",
    "      - Statistics about the tags in the file.\n",
    "      - Statistics about the users. \n",
    "      \n",
    "      \n",
    "   - Data Cleaning:\n",
    "      - Audicting the tags that are usefull for us to answer the questions above.\n",
    "      - Problems encontered during audiction.\n",
    "      - How the problems are going to be fixed.\n",
    "      \n",
    "      \n",
    "   - Data Wrangling:\n",
    "      - Convert the data to JSON.\n",
    "      - Fix the problems reported in Data Cleaning stage.\n",
    "      - Adds json to MongoDB database.\n",
    "      - Basic statistics about the database.\n",
    "   \n",
    "   \n",
    "   - Drawing conclusions:\n",
    "      - Query the database answering the questions above.\n",
    "      \n",
    "      \n",
    "   - Additional Ideas\n",
    "      - Ideas from the process and how it is possible to improve the data.\n",
    "\n",
    "### Dataset\n",
    "   [Region choosed: São Paulo, Brazil](https://mapzen.com/data/metro-extracts/metro/sao-paulo_brazil/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"sao-paulo_brazil.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Getting file size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size im mega bytes:\n",
      "1081.490417\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"File size im mega bytes:\")\n",
    "print(os.stat(DATASET_PATH).st_size / 1000000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the size of the document, initially we worked with a scaled down version of the dataset, only 50 mb. Eventually when all the important features had been audicted, we started processing all the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block of code was originally provided so we can reduce the original dataset. The parameter K can be tunned and controls\n",
    "the number of k-th elements that we are going to have in the sampled dataset. k = 100 was the value that we found that gives us\n",
    "a dataset with 10mb of size, this size is suitable for test and debuging.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "#set the path to our entire dataset to OSM_FILE and the sample file to SAMPLE_FILE.\n",
    "OSM_FILE = DATASET_PATH  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "#Set this variable to the dataset that you want to process.\n",
    "DATASET = DATASET_PATH\n",
    "\n",
    "k = 100 #arameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    #OSM HEADER, each file contains a header and a footer that identifies the type of file, this is what defines OSM files.\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    #OSM FOOTER\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting and getting all tags in the dataset, this data is going to give a notion of how many primitives we are dealing with and can be further compared with the data that is going to be imported to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 80672,\n",
      " 'nd': 6467712,\n",
      " 'node': 4763397,\n",
      " 'osm': 1,\n",
      " 'relation': 13278,\n",
      " 'tag': 2157283,\n",
      " 'way': 649248}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def count_tags(filepath):\n",
    "    \"\"\"\n",
    "    This method iterates through all the dataset and counts each time a tag appears.\n",
    "    input: Dataset path\n",
    "    output: Dictionary were key is a tag and value is frequency of that tag.\n",
    "    \"\"\"\n",
    "    tags = {}\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.iter()\n",
    "    for child in root:\n",
    "        if child.tag in tags:\n",
    "            tags[child.tag] += 1\n",
    "        else:\n",
    "            tags[child.tag] = 1\n",
    "    return tags\n",
    "\n",
    "tags = count_tags(DATASET)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data tags definitions\n",
    "   * node: Represents a point in space, it contains an id, latitude and longitude. Most commonly used to delimit a way. It can also contain tag with key and value pair.  \n",
    "   * way : is a ordered list of nodes, normally has at least one tag. Represents ways of going from one place to another. tags nd are reference to node.  \n",
    "   * relation: is one of the core data elements that consists of one or more tags and also an ordered list of one or more nodes, ways and/or relations as members which is used to define logical or geographic relationships between other elements.\n",
    "   * member: node or way that belongs to a relation.  \n",
    "   * nd : reference to node  \n",
    "   * osm : top level tag\n",
    "   * bounds : region bounds  \n",
    "   \n",
    "   \n",
    "   * tag: A tag consists of two items, a key and a value. Tags describe specific features of map elements (nodes, ways, or relations) or changesets. Both items are free format text fields, but often represent numeric or other structured items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tags description, we can see that all features about a node, way or relation are inside tag \"tag\". So we need to take a look in which features we have access and which feature can help us to answer the questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in way:\n",
      "684\n",
      "Number of features in node:\n",
      "584\n",
      "Number of features in relation:\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "def get_keys():\n",
    "    \"\"\"\n",
    "    Each map element(nodes, ways or relations) present their attributes in tags, each tag has an attribute that is the key and\n",
    "    a value that belongs to that key. This method is going to iterate throught all the dataset and get each tag that belongs to\n",
    "    each map element.\n",
    "    \"\"\"\n",
    "    tags_way = set()\n",
    "    tags_node = set()\n",
    "    tags_relation = set()\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                tags_way.add(tag.attrib['k'])\n",
    "        if elem.tag == \"node\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                tags_node.add(tag.attrib['k'])\n",
    "        if elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                tags_relation.add(tag.attrib['k'])\n",
    "        elem.clear()\n",
    "    #pprint.pprint(tags_way)\n",
    "    #pprint.pprint(tags_node)\n",
    "    #pprint.pprint(tags_relation)\n",
    "    print(\"Number of features in way:\")\n",
    "    print(len(tags_way))\n",
    "    print(\"Number of features in node:\")\n",
    "    print(len(tags_node))\n",
    "    print(\"Number of features in relation:\")\n",
    "    print(len(tags_relation))\n",
    "    \n",
    "get_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was suppressed because there are a lot of different types, instead of each key I am only showing the number of keys each element has. That is a lot of features. Since only a small portion of this features are used constantly to describe a primitive, only listing the tags does not help, we need to count the tags and sort it in away that it is possible to see with features we can take advantage.\n",
    "\n",
    "Computing how many times each key was used, only showing the top 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building: 361659\n",
      "source: 341858\n",
      "height: 333240\n",
      "highway: 260072\n",
      "name: 182318\n",
      "surface: 87429\n",
      "oneway: 68367\n",
      "addr:street: 41876\n",
      "addr:city: 39255\n",
      "addr:housenumber: 38031\n",
      "addr:postcode: 31785\n"
     ]
    }
   ],
   "source": [
    "def get_count_keys():\n",
    "    \"\"\"\n",
    "    This method counts the amount of times each tag appears and sort it in a decrescent order.\n",
    "    output: dictionary where the key is a tag attribute and value is the frequency of each attribute. \n",
    "    \"\"\"\n",
    "    keys_counter = {}\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] in keys_counter:\n",
    "                    keys_counter[tag.attrib['k']] += 1\n",
    "                else:\n",
    "                    keys_counter[tag.attrib['k']] = 1\n",
    "        elem.clear()\n",
    "    return keys_counter\n",
    "\n",
    "#Sort a dictionary by value in decrescent order and print the top 10\n",
    "keys_counter = get_count_keys()\n",
    "sorter_keys_counter = sorted(keys_counter, key = keys_counter.get, reverse = True)\n",
    "for index, key in enumerate(sorter_keys_counter):\n",
    "    print(\"%s: %s\" % (key, keys_counter[key]))\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oks, now we can pick the features that can help us to answer the question stated in introduction. The features that we are going to use are:\n",
    "   - street: Gives the address\n",
    "   - name: Name of the place\n",
    "   - height: Height of the building\n",
    "   - cousine: type of cousine\n",
    "   - amenity: type of community facilite\n",
    "   - suburb: In a city we can have multiple place with the same adress, suburb subdivides a city.\n",
    "   - aeroway: The aeroway key is used to tag physical infrastructure used to support aircraft, air travel, spacecraft and space flight, in particular the elements associated with airports, spaceports and heliports etc.\n",
    "   - building: The building key is used to mark areas as a building.\n",
    "   - city\n",
    "   - postcode\n",
    "\n",
    "For more information about tag go to: http://wiki.openstreetmap.org/wiki/  \n",
    "\n",
    "   Some of this data need cleaning and others no. Features like street, name, height, suburb, city and postcode are open features where the user is free to enter what he things is suitable, because of that a lot of human errors can happen here. Now features like cousine, amenity, aeroway and building already have categories that the user can choose, it is also prune to erros but the chance of having fields with wrong spelling or weird caracters is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, computing the number of users that we are dealing with, if we have a lot of user that contributed equally the chances ar that the data is going to have a lot of inconsistences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonix-Mapper: 2802473\n",
      "AjBelnuovo: 393998\n",
      "Bonix-Importer: 342773\n",
      "cxs: 168544\n",
      "O Fim: 115462\n",
      "johnmogi: 98863\n",
      "MCPicoli: 94870\n",
      "ygorre: 89844\n",
      "naoliv: 80755\n",
      "patodiez: 74544\n",
      "Roberto Costa: 64703\n"
     ]
    }
   ],
   "source": [
    "def users(filename):\n",
    "    \"\"\"\n",
    "    This method gets all attributes equal to user and count the frequency each user appears\n",
    "    input: File path\n",
    "    output: Dictionary where keys are users and value is the amount of times each user appears. \n",
    "    \"\"\"\n",
    "    users = {}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if  \"user\" in element.attrib:\n",
    "            if element.attrib['user'] in users:\n",
    "                users[element.attrib['user']] += 1\n",
    "            else:\n",
    "                users[element.attrib['user']] = 1\n",
    "        element.clear()\n",
    "    return users\n",
    "\n",
    "#Sort a dictionary by value in decrescent order and print the top 10\n",
    "data_users = users(DATASET)\n",
    "sorter_data_users = sorted(data_users, key = data_users.get, reverse = True)\n",
    "for index, key in enumerate(sorter_data_users):\n",
    "    print(\"%s: %s\" % (key, data_users[key]))\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistics about user data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data from Bonix-Mapper:\n",
      "51.6497008896\n",
      "Percentage of data from the top 5 contributer:\n",
      "70.4626659833\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of data from Bonix-Mapper:\")\n",
    "print(float(data_users[sorter_data_users[0]])/float(sum(data_users.values()))*100.0)\n",
    "\n",
    "print(\"Percentage of data from the top 5 contributer:\")\n",
    "print(float(sum(data_users[user] for user in sorter_data_users[:5]))/float(sum(data_users.values()))*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we can see that 51% of the data comes from one user, this data probably can be trusted and follows a certain pattern. Also 70% of the data comes from the top 5 contributers, it may contain a little inconsistency beetween than.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "As stated before, the fields that needs audiction are:\n",
    "   - street\n",
    "   - height\n",
    "   - suburb\n",
    "   - city\n",
    "   - postcode\n",
    "   - name\n",
    "\n",
    "#### Audicting street name\n",
    "\n",
    "Audict addr:street from way tag\n",
    "Standartization: All strings are going to be converted to ascii and all letters to lower case, easier way to deal with accents and upper case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import sys\n",
    "import unidecode\n",
    "\n",
    "def audict_addr_street():\n",
    "    \"\"\"\n",
    "    This method audicts addr:street, it has a dictionary that contains the expected types of streets,\n",
    "    these types of street are compared against the first word of addr:street, if any type fits, the \n",
    "    address is normal, otherwise it has to be fixed.\n",
    "    output: Streets that needs to be fixed.\n",
    "    \"\"\"\n",
    "    street_first_word_re = re.compile(r\"^[a-zA-Z]*\", re.UNICODE)\n",
    "    street_types = collections.defaultdict(set)\n",
    "    expected_street_types = [\"rua\", \"avenida\", \"alameda\", \"via\", \"estrada\", \"rodovia\", \"praca\", \"viaduto\", \"travessa\", \"largo\",\n",
    "                            \"passagem\", \"marginal\", \"parque\", \"acesso\", \"vila\", \"ponte\", \"ladeira\", \"viela\", \"marina\", \"rotatoria\"]\n",
    "\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if  elem.tag == \"node\" or elem.tag == \"way\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == \"addr:street\"):\n",
    "                    #print(tag.attrib['v'])\n",
    "                    m = street_first_word_re.findall(unidecode.unidecode(tag.attrib['v']))\n",
    "                    if m[0]:\n",
    "                        m[0] = m[0].lower()\n",
    "                        if m[0] not in expected_street_types:\n",
    "                            street_types[m[0]].add(unidecode.unidecode(tag.attrib['v']).lower())\n",
    "            elem.clear()\n",
    "    return street_types\n",
    "    \n",
    "audict_addr_street = audict_addr_street()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of audict_addr_street is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'av cassiano ricardo',\n",
       " 'av jacu pessego / nova trabalhadores',\n",
       " 'av sara veloso',\n",
       " 'av. 9 de abril',\n",
       " 'av. agenor c. de magalhaes',\n",
       " 'av. albert bartholome',\n",
       " 'av. andromeda',\n",
       " 'av. antonio joaquim de moura andrade',\n",
       " 'av. arlindo betio',\n",
       " 'av. augusto zorzi baradel furquim',\n",
       " 'av. brg. faria lima',\n",
       " 'av. consolacao, 1290',\n",
       " 'av. das nacoes unidas',\n",
       " 'av. francisco nobrega barbosa',\n",
       " 'av. horacio lafer',\n",
       " 'av. liberdade',\n",
       " 'av. marginal',\n",
       " 'av. mathias lopes',\n",
       " 'av. yojiro takaoka'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audict_addr_street[\"av\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Initially, for the SAMPLE part of the data, only two problems were reported:  \n",
    "   - \"Av.\" is apreviation for \"Avenida\", that is going to be fixed programmaticaly.\n",
    "   - \"JOSÉ PEREIRA CRUZ\", searching for it on google maps we see that is classified as \"Rua\". This problem is going to be fixed manually, but if there are too many similar cases in the entire dataset, that is going to became a huge time consumption task.\n",
    "   \n",
    "When audicting all the dataset, new problems emerged:\n",
    "   - new street types that were not included in the street_tyles list like, marginal, parque, acesso.\n",
    "   - soma street types with typos, like: alamedas, rodoanel and rue.\n",
    "   - A lot of differences because of accentuation and lower or upper case letters.\n",
    "   - Some streets without a type. For all the street types that can be found in Brazil take a look at:  \n",
    "   http://coopus.com.br/site/upload/tabelas-ans/lodragouro-tipo.pdf\n",
    "   \n",
    "Solution encontered for the problems:\n",
    "   - All new street types that were not in the list and are present in the document lodragouro-tipo.pdf where add to the list.\n",
    "   - Street types with typos are going to be ignored.\n",
    "   - All characteres are going to be converted to ASCII.\n",
    "   - Streets without a type are going to be ignored.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_name_problem = collections.defaultdict(set)\n",
    "street_name_problem['avenida'].add(\"av.\")\n",
    "street_name_problem['alamedas'].add(\"al.\")\n",
    "street_name_problem['rua'].add(\"r.\")\n",
    "street_name_problem['rodovia'].add(\"rod.\")\n",
    "street_name_problem['estrada'].add(\"estr.\")\n",
    "\n",
    "def fix_street(street_name, street_name_problem):\n",
    "    \"\"\"\n",
    "    This method receives a street name, removes upper letter and converts each caracter to utf-8.\n",
    "    input: street_name that is the street that is going to be evaluated.\n",
    "           street_name_problem is a dictionaty with type of words that needs to be fixed.\n",
    "    \"\"\"\n",
    "    street_name = unidecode.unidecode(street_name).lower()\n",
    "    street_type = street_name.split(\" \")[0].encode(\"utf-8\")\n",
    "    for correct_street_types in street_name_problem:\n",
    "        for wrong_street_type in street_name_problem[correct_street_types]:\n",
    "            if (street_type == wrong_street_type):\n",
    "                street_name = street_name.replace(street_type, correct_street_types)\n",
    "    return street_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audicting name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rodoanel mario covas\n",
      "rua sem denominacao\n",
      "rodovia raposo tavares\n",
      "rodovia presidente dutra\n",
      "avenida corifeu de azevedo marques\n",
      "rodovia anchieta\n",
      "itau\n",
      "bradesco\n",
      "santander\n",
      "rua um\n",
      "banco do brasil\n"
     ]
    }
   ],
   "source": [
    "def audict_name():\n",
    "    \"\"\"\n",
    "    This method audicts name, this variable contains names of streets and places. We do not need to \n",
    "    audict for street because we already did it for addr:street. Regarding name of places, we are going\n",
    "    to remove accents and upper case letters.\n",
    "    output: Dictionary with the frequency each time a name of a place appears in the dataset.\n",
    "    \"\"\"\n",
    "    names = {}\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if  elem.tag == \"node\" or elem.tag == \"way\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == \"name\"):\n",
    "                    name = unidecode.unidecode(tag.attrib['v']).lower()\n",
    "                    if name in names:\n",
    "                        names[name] += 1\n",
    "                    else:\n",
    "                        names[name] = 1\n",
    "                   \n",
    "        elem.clear()\n",
    "    return names\n",
    "    \n",
    "#limiting the output of audict_name to only show the top 10 with the same name.\n",
    "names = audict_name()\n",
    "names = sorted(names, key = names.get, reverse = True)\n",
    "for index, name in enumerate(names):\n",
    "    print(name)\n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of audict name was limited because it is a very long list, the ouput contains the top 10 names that appears more in the dataset. Name has the same properties as street, so the same code can be used to fix it.\n",
    "\n",
    "#### Audicting height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audict_height():\n",
    "    \"\"\"\n",
    "    This method audicts height, it searchs for number that fit a pattern. The pattern used is\n",
    "    a string that only has numbers and one \".\" to separete from decimal part. Any string that does not\n",
    "    fit in the pattern is going to be considered a height that needs to be fixed.\n",
    "    output: Array containing two dictionaries, the first contains the heights that fit the pattern and\n",
    "    the second the heights that does not fit in the pattern.\n",
    "    \"\"\"\n",
    "    heights = []\n",
    "    heights_with_problems = []\n",
    "    isnumber = re.compile(r\"[+-]?\\d+(?:\\.\\d+)?\", re.IGNORECASE)\n",
    "\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == \"height\"):\n",
    "                    m = isnumber.findall(tag.attrib['v'])\n",
    "                    if m:\n",
    "                        if(m[0] == tag.attrib['v']):\n",
    "                            heights.append(tag.attrib['v'])\n",
    "                        else:\n",
    "                            heights_with_problems.append(tag.attrib['v'])\n",
    "        elem.clear()\n",
    "    return [heights, heights_with_problems]\n",
    "\n",
    "#Print the heights with problem.\n",
    "#pprint.pprint(audict_height()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of audict height was supressed, the results where summarized below.\n",
    "\n",
    "Height: The default unit is meters. If the height is measured in a different unit, the unit abbreviation is appended to the value, separated by a space.\n",
    "initially we only had two problem with height:\n",
    "   - Values with unit.\n",
    "   - Values that are not in meters.\n",
    "\n",
    "When audicting all the dataset\n",
    "   - Measures with \",\" as separator and others with \".\".\n",
    "   - Building with multiple parts.\n",
    "\n",
    "Solution for the problems listed above:\n",
    "   - Remove any num numerical character\n",
    "   - Keep all measures in meters\n",
    "   - Convert \",\" to \".\"\n",
    "   - Convert the data to a list of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_height(height_from_data):\n",
    "    \"\"\"\n",
    "    This method receives a string that represents a height and fix test it agains the problems found in\n",
    "    the audiction fase, if the problem is present we fix it.\n",
    "    input: String that represents a height.\n",
    "    output: Array of heights in float, it is an array because a building can have multiple heights.\n",
    "    \"\"\"\n",
    "    heights = []\n",
    "    height_from_data = height_from_data.split(\";\")\n",
    "    for height in height_from_data:\n",
    "        height = height.replace(\" \",\"\").replace(\",\",\".\").replace(\"m\",\"\")\n",
    "        if \"'\" in height:\n",
    "            height = float(height.replace(\"'\",\"\"))\n",
    "            height = height * 0.3048\n",
    "        else:\n",
    "            height = float(height)\n",
    "        heights.append(height)\n",
    "    return heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audicting city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audict_city():\n",
    "    \"\"\"\n",
    "    This method audicts city, it basically counts each occurency of city. That is usefull so we can see with problems we have\n",
    "    in the data. We also remove upper case letters and convert the strings to utf-8\n",
    "    output: Dictionary where keys are the cities and values is the amount of times each city appears.\n",
    "    \"\"\"\n",
    "    cities = {}\n",
    "\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == \"addr:city\"):\n",
    "                    city = unidecode.unidecode(tag.attrib['v']).lower()\n",
    "                    if city in cities:\n",
    "                        cities[city] += 1\n",
    "                    else:\n",
    "                        cities[city] = 1\n",
    "            elem.clear()\n",
    "    return cities\n",
    "\n",
    "#To run audict_city, run the code below.\n",
    "#pprint.pprint(audict_city())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems encontered for cities:\n",
    "   - Lowercase vs uppercases\n",
    "   - utf-8 to ascii\n",
    "   - A lot of misspeling words and wrong names, this data can not be fixed programmatically\n",
    "       - sao paulol\n",
    "   - 2 inputs with postal code instead of city name, to fix it the postal code is going to be replaced by the city name.\n",
    "       - 06097-100\n",
    "   - same cities with state initials on the name, to fix it everything after \"-\" and \",\" is going to be ignored.\n",
    "   - every caracter that is not a letter is going to be removed\n",
    "       - campo0 limpo paulista\n",
    "   - city name with \"sp\" are going to be removed without regular expression.\n",
    "       - varzea paulista, sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_postal_code_to_city = {}\n",
    "dict_postal_code_to_city[\"06097-100\"] = \"osasco\"\n",
    "dict_postal_code_to_city[\"08589-000\"] = \"itaquaquecetuba\"\n",
    "\n",
    "def fix_city(city_name, dict_postal_code_to_city):\n",
    "    \"\"\"\n",
    "    This method fix city based on the problems found in the audiction. It removes unwanted caracter leaving only letters and\n",
    "    write spaces. It also converts CEP to city names.\n",
    "    input: city_name that contains the city name.\n",
    "           dict_postal_code_to_city it is a dictionary that contains CEP and its respectives cities.\n",
    "    \"\"\"\n",
    "    city_name = unidecode.unidecode(city_name).lower()\n",
    "    city_name = city_name.replace(\"-sp\",\"\")\n",
    "    city_name = city_name.replace(\", sp\",\"\")\n",
    "    city_name = city_name.replace(\" - sp\",\"\")\n",
    "    if city_name in dict_postal_code_to_city:\n",
    "        city_name = dict_postal_code_to_city[city_name]\n",
    "        return city_name\n",
    "    copy_city_name = city_name\n",
    "    for char in city_name:\n",
    "        if not char.isalpha() and char != \" \":\n",
    "            copy_city_name = copy_city_name.replace(char, \"\")\n",
    "    return copy_city_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audicting postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audict_postcode():\n",
    "    \"\"\"\n",
    "    This method audicts postcode. It compares agains a pattern that represents a postcode in the format\n",
    "    XXXXX-XXX. If postal code does not fits the pattern, it is printed.\n",
    "    output: all postal codes that follows the pattern.\n",
    "    \"\"\"\n",
    "    postcode = {}\n",
    "    postal_code_re = re.compile(r'(\\d{5})([ ])?([-])?([ ])?(\\d{3})', re.IGNORECASE)\n",
    "\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == 'addr:postcode'):\n",
    "                    m = postal_code_re.search(tag.attrib['v'])\n",
    "                    if m:\n",
    "                        m = m.groups()\n",
    "                        postal_code = m[0] + \"-\" + m[4]\n",
    "                        if postal_code in postcode:\n",
    "                            postcode[postal_code] += 1\n",
    "                        else:\n",
    "                            postcode[postal_code] = 1\n",
    "                    else:\n",
    "                        print(tag.attrib['v'])\n",
    "        elem.clear()\n",
    "    return postcode\n",
    "    \n",
    "#To run audict_postcode, run the code below.\n",
    "#audict_postcode = audict_postcode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The postal code that does not match the regular expressions are going to be dropped.\n",
    "Problems with postcode:\n",
    "   - some has hifen others no\n",
    "       - 12.216-540\n",
    "       - 061 5000\n",
    "   - some have white spaces\n",
    "   - \".\" instead of \"-\"\n",
    "   - misplacement of the separator\n",
    "   - wrong number of digits - in this case the data is going to be ignored.\n",
    "   \n",
    "All the problem reported were because the string did not match the standart for postal code:  \n",
    "https://www.correios.com.br/para-voce/precisa-de-ajuda/o-que-e-cep-e-por-que-usa-lo/estrutura-do-cep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_postal_code(postal_code):\n",
    "    \"\"\"\n",
    "    This method receives a postal code and return it in the pattern XXXXX-XXX. If the lenght of the\n",
    "    postal code is not 9, it returns None, invalidating the data.\n",
    "    input: String containing the postal code.\n",
    "    output: String following the postal code pattern.\n",
    "    \"\"\"\n",
    "    postal_code_re = re.compile(r'(\\d{5})([ ])?([-])?([ ])?(\\d{3})', re.IGNORECASE)\n",
    "    m = postal_code_re.search(postal_code)\n",
    "    if m:\n",
    "        m = m.groups()\n",
    "        postal_code = m[0] + \"-\" + m[4]\n",
    "        return postal_code\n",
    "    else:\n",
    "        copy_postal_code = postal_code\n",
    "        for char in postal_code:\n",
    "            if not char.isdigit():\n",
    "                copy_postal_code = copy_postal_code.replace(char, \"\")\n",
    "        copy_postal_code = copy_postal_code[:5] + \"_\" + copy_postal_code[5:]\n",
    "        if len(copy_postal_code) != 9:\n",
    "            return None\n",
    "        else:\n",
    "            return copy_postal_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audicting suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audict_suburb():\n",
    "    \"\"\"\n",
    "    This method audicts suburb. It iterates thought all the addr:suburb and counts all the occurrencies\n",
    "    of each suburb, it also converts all upper letters to lower letters and converts it to utf-8. The \n",
    "    goal is to visualize all suburb so we can isolate the problems that we need to fix.\n",
    "    output: Dictionary where key is suburb and values is how often each key appeared.\n",
    "    \"\"\"\n",
    "    suburbs = {}\n",
    "\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib['k'] == \"addr:suburb\"):\n",
    "                    suburb = unidecode.unidecode(tag.attrib['v']).lower()\n",
    "                    if suburb in suburbs:\n",
    "                        suburbs[suburb] += 1\n",
    "                    else:\n",
    "                        suburbs[suburb] = 1\n",
    "        elem.clear()\n",
    "    return suburbs\n",
    "    \n",
    "#To run audict_suburb, run the code below.\n",
    "#pprint.pprint(audict_suburb())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems founded:\n",
    "   - Some abreviation like parque to pq.\n",
    "       - vl da saude\n",
    "   - Inconsistency because of accents and upper case letters.\n",
    "   - Miss spelling words.\n",
    "       - bras cubas and braz cubas\n",
    "   - One case of a postal code in the data.\n",
    "       - 02924-000\n",
    "   \n",
    "Solutions:\n",
    "   - Convert the abreviations.\n",
    "   - Convert all the characters to ASCII and lower case.\n",
    "   - Searching on google the suburb that corresponds to the postal code is Vila Arcádia. Since it is only one entry, the data is going to be fixed programatically.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suburb_problem = collections.defaultdict(set)\n",
    "suburb_problem['parque'].add(\"pq.\")\n",
    "suburb_problem['jardim'].add(\"jd.\")\n",
    "suburb_problem['vila'].add(\"vl\")\n",
    "\n",
    "dict_postal_code_to_city = {}\n",
    "dict_postal_code_to_city[ \"02924-000\"] = \"vila arcadia\"\n",
    "\n",
    "def fix_suburb(suburb, suburb_problem):\n",
    "    \"\"\"\n",
    "    This method fix suburb. It receives a string and compares if is a postal code, if it is a postal\n",
    "    converts it to its respective surburb name. It also test to see if the type of the suburb is not\n",
    "    abreviated, if it is abreviated we convert it to its full name.\n",
    "    input: suburb is a string containing the name of the suburb.\n",
    "           suburb_problem is a dictionary that contains abreviation of suburb types.\n",
    "    output: String containing the suburb name fixed.\n",
    "    \"\"\"\n",
    "    if suburb in dict_postal_code_to_city:\n",
    "        return dict_postal_code_to_city[suburb]\n",
    "    suburb = unidecode.unidecode(suburb).lower()\n",
    "    suburb_type = suburb.split(\" \")[0].encode(\"utf-8\")\n",
    "    for correct_suburb_types in suburb_problem:\n",
    "        for wrong_suburb_type in suburb_problem[correct_suburb_types]:\n",
    "            if (suburb_type == wrong_suburb_type):\n",
    "                suburb = suburb.replace(suburb_type, correct_suburb_types)\n",
    "    return suburb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs:\n",
      "5425923\n",
      "Number of inputs that are nodes:\n",
      "4763397\n",
      "Number of inputs that are ways:\n",
      "649248\n",
      "Number of inputs that are relations:\n",
      "13278\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The task here is to wrangle the data and change its format to JSON so it can be inserted in MongoDb.\n",
    "For example, for primitives like nodes the output should be like:\n",
    "\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "\n",
    "Features like address have a class of keys like adreess:street, for address only the output is going to be like:\n",
    "\n",
    "{...\n",
    "\"address\": {\n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "\n",
    "- for \"way\" specifically:\n",
    "\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "\n",
    "is going to be turned into\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "\n",
    "- for \"relations\"specifically:\n",
    "  <member ref=\"188909336\" role=\"outer\" type=\"way\" />\n",
    "  <member ref=\"42846055\" role=\"outer\" type=\"way\" />\n",
    "  <member ref=\"42845680\" role=\"outer\" type=\"way\" />\n",
    "  <member ref=\"42847596\" role=\"outer\" type=\"way\" />\n",
    "  \n",
    "  the output is going to be a list of dictionaries like:\n",
    "  \"member\": [{\"ref\":\"188909336, \"role\":\"outer\" \"type\":\"way\"},{...}...]\n",
    "\"\"\"\n",
    "from exceptions import TypeError, ValueError\n",
    "\n",
    "def audict_tag(key, value):\n",
    "    \"\"\"\n",
    "    This function receives a tag that is the key and a value that belongs to that key. It checks if\n",
    "    the tag is one of our attributes of interrest that are: height, street, name, city, postcode and \n",
    "    suburb. If a tag is a tag of interrest, the value is going to be fixed by the functions created \n",
    "    the audiction fase.\n",
    "    input: key that is an attribute.\n",
    "           value that represents the attribute.\n",
    "    output: value fixed.\n",
    "    \"\"\"\n",
    "    if key == \"height\":\n",
    "        value = fix_height(value)\n",
    "    if key == \"street\":\n",
    "        value = fix_street(value, street_name_problem)\n",
    "    if key == \"name\":\n",
    "        value = fix_street(value, street_name_problem)\n",
    "    if key == \"city\":\n",
    "        value = fix_city(value, dict_postal_code_to_city)\n",
    "    if key == \"postcode\":\n",
    "        value = fix_postal_code(value)\n",
    "    if key == \"suburb\":\n",
    "        value = fix_suburb(value, suburb_problem)\n",
    "    return value\n",
    "\n",
    "def add_tag_to_dict(dictionary, key, value):\n",
    "    \"\"\"\n",
    "    This method adds a tag to a dictionary. If the key presents \".\" in the name, it is going to be\n",
    "    removed because mongoDb does not accept dictionary keys with dots in the name.\n",
    "    input: dictionary tag is going to receive the new attribute and its value\n",
    "           key to be add to the dictionary\n",
    "           value that represents the key.\n",
    "    \"\"\"\n",
    "    value = audict_tag(key, value)\n",
    "    dictionary[key.replace(\".\",\"\")] = value\n",
    "\n",
    "def process_addr_key(tag, dictionary):\n",
    "    \"\"\"\n",
    "    Method that receives a tag that belongs to the addr class of attributes and process it.\n",
    "    The method is going to remove the \"addr:\" parte of the attribute name and send the rest to be added\n",
    "    to the dict.\n",
    "    input: tag that contains an attribute key and its value\n",
    "           dictionary that is going to receive the new tag.\n",
    "    \"\"\"\n",
    "    add_tag_to_dict(dictionary, tag.attrib['k'].split(\":\")[1], tag.attrib['v'])\n",
    "        \n",
    "def process_tag(tag, dictionary):\n",
    "    \"\"\"\n",
    "    This method  receives a tag and checks to see if it belongs to the \"addr\" class, if so, it sends it to process_addr_key,\n",
    "    otherwise it sends it to add_tag_to_dict directly.\n",
    "    input: tag with key and value\n",
    "           dictionary that the tag is going to be inserted.\n",
    "    \"\"\"\n",
    "    if (\":\" in tag.attrib['k']):\n",
    "        if (tag.attrib['k'].split(\":\")[0] == \"addr\"):\n",
    "            if \"addr\" not in dictionary:\n",
    "                dictionary[\"addr\"] = {}\n",
    "            process_addr_key(tag, dictionary[\"addr\"])\n",
    "        else:\n",
    "            add_tag_to_dict(dictionary, tag.attrib['k'].replace(\":\", \"_\"), tag.attrib['v'])\n",
    "    else:\n",
    "        if tag.attrib['k'] != \"addr\":\n",
    "            add_tag_to_dict(dictionary, tag.attrib['k'], tag.attrib['v'])\n",
    "\n",
    "def wrangle_node(elem):\n",
    "    \"\"\"\n",
    "    Wrangle node receives a element that is a node, creates a dictionaty and process all items and tags in the node.\n",
    "    input: node element\n",
    "    output: dictionary with all the node items and tags in it.\n",
    "    \"\"\"\n",
    "    node_dict = {}\n",
    "    node_dict[\"primitive\"] = \"node\"\n",
    "\n",
    "    for name, value in elem.items():\n",
    "        node_dict[name] = value\n",
    "\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        process_tag(tag, node_dict)\n",
    "    return node_dict\n",
    "\n",
    "def wrangle_way(elem):\n",
    "    \"\"\"\n",
    "    Wrangle way receives a element that is a node, creates a dictionaty and process all items, nd and tags in the way.\n",
    "    input: way element\n",
    "    output: dictionary with all the way items, nd and tags in it.\n",
    "    \"\"\"\n",
    "    way_dict = {}\n",
    "    way_dict[\"primitive\"] = \"way\"\n",
    "\n",
    "    for name, value in elem.items():\n",
    "        way_dict[name] = value\n",
    "\n",
    "    for nd in elem.iter(\"nd\"):\n",
    "        for name, value in nd.items():\n",
    "            if name in way_dict:\n",
    "                way_dict[name].append(value)\n",
    "            else:\n",
    "                way_dict[name] = []\n",
    "                way_dict[name].append(value)\n",
    "\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        process_tag(tag, way_dict)\n",
    "    return way_dict\n",
    "\n",
    "def wrangle_relation(elem):\n",
    "    \"\"\"\n",
    "    Wrangle relation receives a element that is a relation, creates a dictionaty and process all items, members and tags in the \n",
    "    relation.\n",
    "    input: relation element\n",
    "    output: dictionary with all the relation items, members and tags in it.\n",
    "    \"\"\"\n",
    "    relation_dict = {}\n",
    "    relation_dict[\"primitive\"] = \"relative\"\n",
    "\n",
    "    for name, value in elem.items():\n",
    "        relation_dict[name] = value\n",
    "\n",
    "    member_list = []    \n",
    "    for nd in elem.iter(\"member\"):\n",
    "        member_dict = {}\n",
    "        for name, value in nd.items():\n",
    "            member_dict[name] = value\n",
    "        member_list.append(member_dict.copy())\n",
    "    if len(member_list) != 0:\n",
    "        relation_dict[\"member\"] = member_list\n",
    "\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        process_tag(tag, relation_dict)\n",
    "    return relation_dict\n",
    "\n",
    "def wrangle(elem):\n",
    "    \"\"\"\n",
    "    This methods receives a element, checks to see if it is a way, node or relation and sents it to its proper method to be \n",
    "    processed.\n",
    "    input: element\n",
    "    output: Dictionary with the element attributs in it.\n",
    "    \"\"\"\n",
    "    if elem.tag == \"way\":\n",
    "        json = wrangle_way(elem)\n",
    "        return json\n",
    "    if elem.tag == \"node\":\n",
    "        json = wrangle_node(elem)\n",
    "        return json\n",
    "    if elem.tag == \"relation\":\n",
    "        json = wrangle_relation(elem)\n",
    "        return json\n",
    "    return None\n",
    "\n",
    "def insert_json(json, db):\n",
    "    \"\"\"\n",
    "    Insert json to database\n",
    "    \"\"\"\n",
    "    db.open_street_map.insert_one(json)\n",
    "\n",
    "def query_open_street_map_dataset(db):\n",
    "    print(\"Number of inputs:\")\n",
    "    print(db.open_street_map.find().count())\n",
    "    \n",
    "    print(\"Number of inputs that are nodes:\")\n",
    "    print(db.open_street_map.find({\"primitive\":\"node\"}).count())\n",
    "    \n",
    "    print(\"Number of inputs that are ways:\")\n",
    "    print(db.open_street_map.find({\"primitive\":\"way\"}).count())\n",
    "    \n",
    "    print(\"Number of inputs that are relations:\")\n",
    "    print(db.open_street_map.find({\"primitive\":\"relative\"}).count())\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Connecting to local database, if there is a database with the same name drops and create another.\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient(\"mongodb://localhost:27017\")\n",
    "    client.drop_database(\"open_street_map\")\n",
    "    db = client.open_street_map\n",
    "\n",
    "    #Process every element of the dataset\n",
    "    for event, elem in ET.iterparse(DATASET, events = (\"start\",)):\n",
    "        json = {}\n",
    "        json = wrangle(elem);\n",
    "        try:\n",
    "            if json is not None:\n",
    "                insert_json(json, db)\n",
    "        except:\n",
    "            print(json)\n",
    "        elem.clear()\n",
    "    query_open_street_map_dataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'storageSize': 410177536.0, u'ok': 1.0, u'avgObjSize': 242.3584610397162, u'views': 0, u'db': u'open_street_map', u'indexes': 1, u'objects': 5425923, u'collections': 1, u'numExtents': 0, u'dataSize': 1315018348.0, u'indexSize': 54652928.0}\n"
     ]
    }
   ],
   "source": [
    "print db.command(\"dbstats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done, all the nodes, ways and relations are in a database. Two problems during the process:\n",
    "   - there is a feature called addr only, because of this variable the code was crashing because it was trying to store a dict in a variable type string.\n",
    "   - MongoDb does not accept keys with \".\", there is one feature with \".\" that was making the code to crash, solution for it was to remove all \".\" from keys.\n",
    "   \n",
    "All the erros above were cought handling the errors using try except statements.\n",
    "\n",
    "Some statistics about the MondoDb collection:\n",
    "   - It's size is significantlly smaller than the original osm file.\n",
    "      * original: 1081.490417 mb\n",
    "      * collection: 410148864 bytes, what gives us 410 mb\n",
    "   - The number of nodes, ways and relations in the collection are the same as in the osm file.\n",
    "  \n",
    "#### Data exploration using MondoDb queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can easily answer the questions states in the beginning.\n",
    "   - How tall are the 5 tallest buildings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$unwind\": \"$height\"},\n",
    "            {\"$match\":{\"building\":{\"$ne\":None}, \"name\":{\"$ne\":None}}},\n",
    "           {\"$sort\":{\"height\":-1}},\n",
    "           {\"$limit\":5}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_tallest_buildings = []\n",
    "for data in query_result:\n",
    "    the_tallest_buildings.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: mirante do vale, building: yes, height: 131\n",
      "name: edificio copan, building: apartments, height: 118\n",
      "name: grande sao paulo, building: yes, height: 114\n",
      "name: instituto do cancer do estado de sao paulo octavio frias de oliveira, building: hospital, height: 106\n",
      "name: cbi esplanada, building: commercial, height: 100\n"
     ]
    }
   ],
   "source": [
    "for data in the_tallest_buildings:\n",
    "    print(\"name: %s, building: %s, height: %d\" % (data[\"name\"], data[\"building\"], data[\"height\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second the dataset, The building \"Mirante do Vale\" is the highiest building in São Paulo, with 131 meters. A quick search on google confirms that this is the tallest building, but second the wikipedia, it has 170 meters.[wikipedia](https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_S%C3%A3o_Paulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - With kind of amenity is most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$ne\":None}}},\n",
    "            {\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}},\n",
    "           {\"$limit\":5}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amenities = []\n",
    "for data in query_result:\n",
    "    amenities.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 3694, u'_id': u'parking'}\n",
      "{u'count': 1867, u'_id': u'fuel'}\n",
      "{u'count': 1635, u'_id': u'restaurant'}\n",
      "{u'count': 1553, u'_id': u'school'}\n",
      "{u'count': 1041, u'_id': u'bank'}\n"
     ]
    }
   ],
   "source": [
    "for data in amenities:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query return that we have 3694 parking lots in São Paulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - With kind of cousine is predominant? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$match\":{\"cuisine\":{\"$ne\":None}}},\n",
    "            {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}},\n",
    "           {\"$limit\":5}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cousines = []\n",
    "for data in query_result:\n",
    "    cousines.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 306, u'_id': u'regional'}\n",
      "{u'count': 192, u'_id': u'burger'}\n",
      "{u'count': 162, u'_id': u'pizza'}\n",
      "{u'count': 82, u'_id': u'japanese'}\n",
      "{u'count': 73, u'_id': u'fish_and_chips'}\n"
     ]
    }
   ],
   "source": [
    "for data in cousines:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Second the data we have 306 restaurants the offers \"regional\" food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - With avenue pass by a large number of subburbs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$match\":{\"addr.street\":{\"$regex\":\"^(avenida)\"}, \"addr.suburb\":{\"$ne\":None}}},\n",
    "            {\"$group\":{\"_id\":{\"street\":\"$addr.street\", \"suburb\":\"$addr.suburb\"}, \"count\":{\"$sum\":1}}},\n",
    "            {\"$group\":{\"_id\":\"$_id.street\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}},\n",
    "            {\"$limit\":10}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avenues = []\n",
    "for data in query_result:\n",
    "    avenues.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 9, u'_id': u'avenida robert kennedy'}\n",
      "{u'count': 9, u'_id': u'avenida das nacoes unidas'}\n",
      "{u'count': 6, u'_id': u'avenida brigadeiro faria lima'}\n",
      "{u'count': 5, u'_id': u'avenida sao joao'}\n",
      "{u'count': 5, u'_id': u'avenida anna costa'}\n",
      "{u'count': 5, u'_id': u'avenida dom pedro i'}\n",
      "{u'count': 5, u'_id': u'avenida antonio piranga'}\n",
      "{u'count': 5, u'_id': u'avenida santos dumont'}\n",
      "{u'count': 5, u'_id': u'avenida moinho fabrini'}\n",
      "{u'count': 5, u'_id': u'avenida getulio vargas'}\n"
     ]
    }
   ],
   "source": [
    "for data in avenues:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Second the dataset, we have two avenues that goes by 9 suburbs each, we have \"avenida robert kennedy\" and avenida das nacoes unidas. A quick serch on google returns that the avenue \"Avenida Sapopemba\" goes by 27 suburbs.[wikipedia](https://pt.wikipedia.org/wiki/Avenida_Sapopemba)  \n",
    "   For this question I used regex to match the first street name as \"avenida\" than i had to use two group statements to get the answer i wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - How many places to land my helicopter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$match\":{\"aeroway\":\"helipad\"}},\n",
    "           {\"$group\":{\"_id\":\"helipad\", \"count\":{\"$sum\":1}}}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aeroways = []\n",
    "for data in query_result:\n",
    "    aeroways.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 455, u'_id': u'helipad'}\n"
     ]
    }
   ],
   "source": [
    "for data in aeroways:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 455 helipads in São Paulo region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - I am a big fan of coffe places, how many coffe places I have im my disposal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = [{\"$match\":{\"$or\":[{\"amenity\":\"cafe\"},{\"cuisine\":\"coffee_shop\"}], \"addr.street\":{\"$ne\":None}}},\n",
    "           {\"$group\":{\"_id\":\"coffe_places\",\"count\":{\"$sum\":1}}}]\n",
    "query_result = db.open_street_map.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coffe_places = []\n",
    "for data in query_result:\n",
    "    coffe_places.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 120, u'_id': u'coffe_places'}\n"
     ]
    }
   ],
   "source": [
    "for data in coffe_places:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   There is 120 coffe places where i can go. To answer this questions I have to use the or operand in match statement because a coffe shop can be defined either on amenity as cafe or in cuisine as coffee_shop. [wiki](http://wiki.openstreetmap.org/wiki/Tag:cuisine%3Dcoffee_shop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and additional ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of that project was to obtain a dataset, clean it, wrangle it and awnswer some questions with data from the dataset. During the process all problems were documented in this notebook. Some data that were audited were not used because it could restrict even more the results if they were included in the queries to the database, features like city and postal code. That could happen because a lot of data is missing from the dataset. The main conclusion about the dataset is that it is incomplete, some questions answered did not match with results from other sources.\n",
    "\n",
    "One idea to fix some of the problems is to use the postal code and geolocation. There are some datasets that gives us geolocation, postal code and address used by the Brazilian post office, the benefits of these datasets is that it comes from a goverment agency and are more reliable. Another advantage is that geolocation is linked with an address and postal code, with this information we can fill gaps in the dataset or use it as reference to check how reliable the information is. The downside of using it is that it does not give information like the type of building, its name and other information that we use and are very common in our day to day life. Another problem with this is in the case of new entries that are not present in the post office dataset, it is not possible to tell if the new information is correct or not.\n",
    "\n",
    "Another idea is to use a spell check to fix some inputs with wrong caracters that endup making the cleaning process harder. One benefit is that we would have more homogeneous data making the process of queries more relatiable. Also these features could be tigh together with the dataset from the post office, that way we could use it as reference when the user is typing a new entry offering tips like the type of street and the postal code based on geolocation. The disavantage of this method is that it is necessary to have a way to say if the word is wrong or not.\n",
    "\n",
    "Moreover, we have that very few users contributed to the dataset, 70% of the data comes from 5 users. To improve the quality of the data and the number of users, google could offer a reward the top users. It could also use other users to verify the data, a lot of people use google maps service to move around the city and each time the user gets to where he wants to go, google could ask if the location is the right one. Also on my analytics, I could have used geolocation over a map. A lot of features could use this technique to give an idea of where things are located. An example could be to plot the geolocation data by users, that can give an idea of the area covered by each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
